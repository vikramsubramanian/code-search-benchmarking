{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from data_utils import create_dataset, create_loader\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_feats(model, tokenizer, data_loader, device, desc='Get feats'):\n",
    "    embeds = []\n",
    "\n",
    "    for text in tqdm(data_loader, total=len(data_loader), desc=desc):\n",
    "        text_input = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model(**text_input)\n",
    "        embed = average_pool(outputs.last_hidden_state, text_input['attention_mask'])\n",
    "\n",
    "        embeds.append(embed)\n",
    "\n",
    "    embeds = torch.cat(embeds, dim=0)\n",
    "\n",
    "    return embeds\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def contrast_evaluation(text_embeds, code_embeds, img2txt):\n",
    "    score_matrix_i2t = text_embeds @ code_embeds.t()\n",
    "    scores_i2t = score_matrix_i2t.cpu().numpy()\n",
    "\n",
    "\n",
    "    ranks = np.ones(scores_i2t.shape[0]) * -1\n",
    "    for index, score in enumerate(scores_i2t):\n",
    "        inds = np.argsort(score)[::-1]\n",
    "        ranks[index] = np.where(inds == img2txt[index])[0][0]\n",
    "\n",
    "    # Compute metrics\n",
    "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "    mrr = 100.0 * np.mean(1 / (ranks + 1))\n",
    "\n",
    "    eval_result = {'r1': tr1,\n",
    "                   'r5': tr5,\n",
    "                   'r10': tr10,\n",
    "                   'mrr': mrr}\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating retrieval dataset\")\n",
    "#change language and path to dataset here\n",
    "_, _, test_dataset, code_dataset = create_dataset('dataset/CSN', 'ruby')\n",
    "\n",
    "test_loader, code_loader = create_loader([test_dataset, code_dataset], [None, None],\n",
    "                                             batch_size=[256, 256],\n",
    "                                             num_workers=[4, 4], is_trains=[False, False], collate_fns=[None, None])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-base\")\n",
    "model = AutoModel.from_pretrained(\"thenlper/gte-base\")\n",
    "\n",
    "print('\\nStart zero-shot evaluation...')\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "text_embeds = get_feats(model, tokenizer, test_loader, device, desc='Get text feats')\n",
    "code_embeds = get_feats(model, tokenizer, code_loader, device, desc='Get code feats')\n",
    "test_result = contrast_evaluation(text_embeds, code_embeds, test_loader.dataset.text2code)\n",
    "print(f'\\n====> zero-shot test result: ', test_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
